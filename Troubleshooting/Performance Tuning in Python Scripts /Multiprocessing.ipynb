{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- company uses Network-Attached Storage (NAS) to store all data generated daily (e.g., videos, photos). \n",
    "\n",
    "- we need to back up the data in the production NAS (mounted at /data/prod on the server) to the backup NAS (mounted at /data/prod_backup on the server). \n",
    "\n",
    "- former member of the team developed a Python script (full path /scripts/dailysync.py) that backs up data daily. But recently, there's been a lot of data generated and the script isn't catching up to the speed. As a result, the backup process now takes more than 20 hours to finish, which isn't efficient at all for a daily backup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would:\n",
    "- Identify what limits the system performance: I/O, Network, CPU, or Memory\n",
    "- Use `rsync` command instead of cp to transfer data\n",
    "- Get system standard output and manipulate the output\n",
    "- Find differences between threading and multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU bound\n",
    "**CPU bound** means the program is bottlenecked by the CPU (Central Processing Unit). When the program is waiting for I/O (e.g., disk read/write, network read/write), the CPU is free to do other tasks, even if the program is stopped. The speed of the program will mostly depend on how fast that I/O can happen; if we want to speed it up, we'll need to speed up the I/O. If the program is running lots of program instructions and not waiting for I/O, then it's CPU bound. Speeding up the CPU will make the program run faster.\n",
    "\n",
    "In either case, the key to speeding up the program might not be to speed up the hardware but to optimize the program to reduce the amount of I/O or CPU it needs. *Or we can have it do I/O while it also does CPU-intensive work.* CPU bound implies that upgrading the CPU or optimizing code will improve the overall computing performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can use `psutil` (process and system utilities) is a cross-platform library for retrieving information on running processes and system utilization (CPU, memory, disks, network, sensors) in Python. It's mainly useful for system monitoring, profiling, and limiting process resources and management of running processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import psutil\n",
    "psutil.cpu_percent()\n",
    ">>> 2.9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that CPU utilization is low. Here, we have a CPU with multiple cores; this means one fully loaded CPU thread/virtual core equals 2.9% of total load. So, it only uses one core of the CPU regardless of having multiple cores -> we noticed that they're not reaching the limit.\n",
    "\n",
    "So, we check the CPU usage, and it looks like the script only uses a single core to run. **But the server has a bunch of cores, which means the task is CPU-bound.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using `psutil.disk_io_counters()` and `psutil.net_io_counters()` we'll get byte read and byte write for disk I/O and byte received and byte sent for the network I/O bandwidth. \n",
    "\n",
    "#### For checking disk I/O, we can use the following command:\n",
    "`psutil.disk_io_counters()`\n",
    "\n",
    "#### For checking the network I/O bandwidth:\n",
    "`psutil.net_io_counters()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics rsync command\n",
    "`rsync`(remote sync) is a utility for efficiently transferring and synchronizing files between a computer and an external hard drive and across networked computers by comparing the modification time and size of files. One of the important features of `rsync` is that it works on the **delta transfer algorithm**, which means *it'll only sync or copy the changes from the source to the destination instead of copying the whole file*. This ultimately reduces the amount of data sent over the network.\n",
    "\n",
    "`rsync [Options] [Source-Files-Dir] [Destination]`\n",
    "\n",
    "Options (common):\n",
    "\n",
    "- -v = Verbose output\n",
    "\n",
    "- -q = Suppress message output\n",
    "\n",
    "- -a = Archive files and directory while synchronizing\n",
    "\n",
    "- -r = Sync files and directories recursively\n",
    "\n",
    "- -b = Take the backup during synchronization\n",
    "\n",
    "- -z = Compress file data during the transfer\n",
    "\n",
    "i.e. \n",
    "- copy or sync files locally `rsync -zvh [Source-Files-Dir] [Destination]`\n",
    "- copy or sync dir locally `rsync -zavh [Source-Files-Dir] [Destination]`\n",
    "- copy files and directories recursively locally - `rsync -zrvh [Source-Files-Dir] [Destination]`\n",
    "```python\n",
    "import subprocess\n",
    "src = \"data/prod\" # replace <source-path> with the source directory\n",
    "dest = \"data/prod_backup\" # replace <destination-path> with the destination directory\n",
    "subprocess.call([\"rsync\", \"-arq\", src, dest]) # would return 0 if no errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing\n",
    "\n",
    "Now, when we go through the hierarchy of the subfolders of `/data/prod`, data is from different projects (e.g., , beta, gamma, kappa) and they're independent of each other. So, in order to efficiently back up parallelly, use multiprocessing to take advantage of the idle CPU cores. Initially, because of CPU bound, the backup process takes **more than 20 hours to finish**, which isn't efficient for a daily backup. Now, by using multiprocessing, we can back up the data from the source to the destination parallelly by utilizing the multiple cores of the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll get the Python script `multisync.py` for practice in order to understand how multiprocessing works. We used the Pool class of the multiprocessing Python module. Here, we define a run method to perform the tasks. Next, we create a pool object of the `Pool class` of a specific number of CPUs on our system has by passing a number of tasks we have. Start each task within the pool object by calling the `map` instance method, and pass the run function and the list of tasks as an argument.\n",
    "\n",
    "`multisync.py `\n",
    "```python\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "from multiprocessing import Pool\n",
    "def run(task):\n",
    "  # Do something with task here\n",
    "    print(\"Handling {}\".format(task))\n",
    "if __name__ == \"__main__\":\n",
    "  tasks = ['task1', 'task2', 'task3']\n",
    "  # Create a pool of specific number of CPUs\n",
    "  p = Pool(len(tasks))\n",
    "  # Start each task within the pool\t\n",
    "  p.map(run, tasks)\n",
    "```\n",
    "\n",
    "`dailysync.py`\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "src = \"/data/prod/\"\n",
    "dest = \"/data/prod_backup/\"\n",
    "subprocess.call([\"rsync\", \"-arq\", src, dest])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**let's fix CPU bound so that it doesn't take more than 20 hours to finish.** We would apply multiprocessing, which takes advantage of the idle CPU cores for parallel processing. \n",
    "\n",
    "### The result of multiprocessing over main folders `['alpha', 'sigma', 'gamma', 'omega', 'beta', 'kappa', 'delta']`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import tqdm\n",
    "import subprocess\n",
    "\n",
    "def run(task):\n",
    "    # Do something with task here\n",
    "    print('current dir',os.getcwd())\n",
    "    src = task #data/prod\n",
    "    dest = 'data/prod_backup/'\n",
    "    subprocess.call([\"rsync\", \"-zavh\", src, dest])\n",
    "    print(\"Handling {}\".format(task))\n",
    "    print('Dest', dest)\n",
    "if __name__ == \"__main__\":\n",
    "    tasks = []\n",
    "    folders = ['alpha', 'sigma', 'gamma', 'omega', 'beta', 'kappa', 'delta']\n",
    "    for folder in folders:\n",
    "        tasks.append(os.path.join('data/prod',folder))\n",
    "\n",
    "    # Create a pool of specific number of CPUs\n",
    "    print(len(tasks))\n",
    "    p = Pool(len(tasks))\n",
    "    # Start each task within the pool     \n",
    "    p.map(run, tasks)\n",
    "    for task in tasks:\n",
    "        print('task',task)\n",
    "    for _ in tqdm.tqdm(p.map(run, tasks), total=len(tasks)):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
